<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="/GreenData/assets/xslt/atom.xslt" ?>
<?xml-stylesheet type="text/css" href="/GreenData/assets/css/atom.css" ?>
<feed xmlns="http://www.w3.org/2005/Atom">
	<id>https://howtoanalyse.github.io/GreenData/</id>
	<title>GreenData</title>
	<updated>2017-12-18T07:07:09+08:00</updated>

	<subtitle>GreenData is a website for wonders worth sharing</subtitle>

	
		
		<author>
			
				<name>Sara Z.</name>
			
			
			
		</author>
	

	<link href="https://howtoanalyse.github.io/GreenData/atom.xml" rel="self" type="application/rss+xml" />
	<link href="https://howtoanalyse.github.io/GreenData/" rel="alternate" type="text/html" />

	<generator uri="http://jekyllrb.com" version="3.3.1">Jekyll</generator>

	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/CategoricalFeatures/</id>
			<title>Feature Engineering</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/CategoricalFeatures/" rel="alternate" type="text/html" title="Feature Engineering" />
			<updated>2017-12-18T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary>Feature Preprocessing and generation with respect to models</summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/CategoricalFeatures/">&lt;p&gt;Ordinal features refer to ordered categorical features. Examples include&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Driver’s license: A,B,C,D&lt;/li&gt;
  &lt;li&gt;Education level: Bachelor, Master, Doctoral&lt;/li&gt;
  &lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;difference-between-numeric-and-ordinal-features-with-values-123&quot;&gt;Difference between Numeric and Ordinal Features with values 1,2,3…&lt;/h3&gt;

&lt;p&gt;For numerical features with values 1,2,3…, we can conclude that the distance between first, and the second class is equal to the distance between second and the third class, but because for ordinal features, we can’t tell which distance is bigger.&lt;/p&gt;

&lt;p&gt;As these numeric features, we can’t sort and integrate an ordinal feature the other way, and expect to get similar performance.&lt;/p&gt;

&lt;h3 id=&quot;encode-a-categorical-feature&quot;&gt;Encode a Categorical Feature&lt;/h3&gt;

&lt;p&gt;The simplest way to encode a categorical feature is to map it’s unique values to different numbers.&lt;/p&gt;

&lt;p&gt;This method works fine with two ways because tree-methods can split feature, and extract most of the useful values in categories on its own.&lt;/p&gt;

</content>

			
				<category term="Machine Learning" />
			
			
				<category term="overview" />
			

			<published>2017-12-18T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/decisionTree/</id>
			<title>Decision Tree</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/decisionTree/" rel="alternate" type="text/html" title="Decision Tree" />
			<updated>2017-12-17T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/decisionTree/">&lt;p&gt;Decision tree classifiers are incredibly simple in theory. In their simplest form, decision tree classifiers ask a series of Yes/No questions about the data — each time getting closer to finding out the class of each entry — until they either classify the data set perfectly or simply can’t differentiate a set of entries. Think of it like a game of Twenty Questions, except the computer is much, much better at it.&lt;/p&gt;

&lt;p&gt;The nice part about decision tree classifiers is that they are scale-invariant, i.e., the scale of the features does not affect their performance.&lt;/p&gt;

&lt;p&gt;A common problem that decision trees face is that they’re prone to overfitting: They complexify to the point that they classify the training set near-perfectly, but fail to generalize to data they have not seen before.
Random Forest classifiers work around that limitation by creating a whole bunch of decision trees (hence “forest”) — each trained on random subsets of training samples (drawn with replacement) and features (drawn without replacement) — and have the decision trees work together to make a more accurate classification.&lt;/p&gt;

&lt;h2 id=&quot;random-forest&quot;&gt;Random Forest&lt;/h2&gt;

&lt;p&gt;In RandomForest model we average n similar performing trees (“forest”), trained independently. So two RF with 1000 trees is essentially the same as single RF model with 2000 trees.&lt;/p&gt;

&lt;h2 id=&quot;gradient-boosted-decision-trees&quot;&gt;Gradient Boosted Decision Trees&lt;/h2&gt;

&lt;p&gt;In GBDT model we have sequence of trees, each improve predictions of all previous.&lt;/p&gt;

&lt;p&gt;So taking other settings the same, dropping the n-th tree has different effects on these two models.&lt;/p&gt;

&lt;p&gt;For Random Forest, the order of trees does not matter in RandomForest and performance drop will be very similar on average.&lt;/p&gt;

&lt;p&gt;For GBDT:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;if we drop first tree, sum of all the rest trees will be biased and overall performance should drop.&lt;/li&gt;
  &lt;li&gt;if we drop the last tree, sum of all previous tree won’t be affected, and therefore performance will change insignificantly (in case we have enough trees)&lt;/li&gt;
&lt;/ul&gt;
</content>

			
				<category term="Machine Learning" />
			
			
				<category term="classification" />
			

			<published>2017-12-17T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine-learning/ensembling/</id>
			<title>Bagging, boosting and stacking in machine learning</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine-learning/ensembling/" rel="alternate" type="text/html" title="Bagging, boosting and stacking in machine learning" />
			<updated>2017-12-16T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine-learning/ensembling/">&lt;p&gt;Bagging and boosting are two families of ensemble methods.&lt;/p&gt;

&lt;p&gt;Ensemble methods aim at combining the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.&lt;/p&gt;

&lt;h6 id=&quot;baggingshort-for-bootstrap-aggregating&quot;&gt;Bagging(short for Bootstrap Aggregating):&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;build several base estimators independently and then to average their predictions.&lt;/li&gt;
  &lt;li&gt;aim to decrease variance by generating additional data for training from the original dataset&lt;/li&gt;
  &lt;li&gt;suitable for models with high variance low bias (complex models)&lt;/li&gt;
  &lt;li&gt;Examples: Random forest, which develop fully grown trees (note that RF modifies the grown procedure to reduce the correlation between trees)&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;boosting&quot;&gt;Boosting:&lt;/h6&gt;

&lt;ul&gt;
  &lt;li&gt;build several base estimators sequentialy and one tries to reduce the bias of the combined estimator&lt;/li&gt;
  &lt;li&gt;aim to decrease bias&lt;/li&gt;
  &lt;li&gt;suitable for models with low variance high bias&lt;/li&gt;
  &lt;li&gt;Examples:  Gradient boosting&lt;/li&gt;
&lt;/ul&gt;

</content>

			
				<category term="Machine-Learning" />
			
			
				<category term="ensemble" />
			

			<published>2017-12-16T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/statvsml/</id>
			<title>What is the difference between data mining, statistics, machine learning and AI?</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/statvsml/" rel="alternate" type="text/html" title="What is the difference between data mining, statistics, machine learning and AI?" />
			<updated>2017-12-15T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/statvsml/">&lt;p&gt;Artificial Intelligence is the study of how to create intelligent agents. Most tasks that require intelligence require an ability to induce new knowledge from experiences. Thus, a large area within AI is machine learning. Procedures in machine learning include ideas derived directly from, or inspired by, classical statistics, but they don’t have to be. Data mining is an area that has taken much of its inspiration and techniques from machine learning (and some, also, from statistics), but aims at either to discover / generate some preliminary insights in an area where there really was little knowledge beforehand, or to be able to predict future observations accurately.&lt;/p&gt;

&lt;p&gt;There are considerable overlaps among them. If we have to make some distinctions, they are:&lt;/p&gt;

&lt;p&gt;Statistics is about &lt;strong&gt;numbers&lt;/strong&gt;. It is mostly employed towards better understanding particular data generating processes. Thus, it usually starts with a formally specified model, and from this are derived procedures to accurately extract that model from noisy instances (i.e., estimation–by optimizing some loss function) and to be able to distinguish it from other possibilities (i.e., inferences based on known properties of sampling distributions). The prototypical statistical technique is regression.&lt;/p&gt;

&lt;p&gt;Data Mining is about using Statistics as well as other programming methods to find &lt;strong&gt;patterns&lt;/strong&gt; hidden in the data so that you can explain some phenomenon. Data Mining builds intuition about what is really happening in some data and is still little more towards math than programming, but uses both. Common data mining techniques would include cluster analyses, classification and regression trees, and neural networks.&lt;/p&gt;

&lt;p&gt;Machine Learning uses Data Mining techniques and other learning algorithms to build models of what is happening behind some data so that it can predict future outcomes. Math is the basis for many of the algorithms, but this is more towards programming. A computer program is said to learn some task from experience if its performance at the task improves with experience, according to some performance measure. Machine learning involves the study of algorithms that can extract information automatically (i.e., without on-line human guidance).&lt;/p&gt;

&lt;p&gt;Artificial Intelligence uses models built by Machine Learning and other ways to reason about the world and give rise to intelligent behavior whether this is playing a game or driving a robot/car. Artificial Intelligence has some goal to achieve by predicting how actions will affect the model of the world and chooses the actions that will best achieve that goal. Very programming based.&lt;/p&gt;

</content>

			
				<category term="Machine Learning" />
			
			

			<published>2017-12-15T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/cv/</id>
			<title>What is the difference between test set and validation set?</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/cv/" rel="alternate" type="text/html" title="What is the difference between test set and validation set?" />
			<updated>2017-12-14T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/cv/">&lt;p&gt;When you have a large data set, it’s recommended to split it into 3 parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Training set: This is used to build up our prediction algorithm. Our algorithm tries to tune itself to the quirks of the training data sets. In this phase we usually create multiple algorithms in order to compare their performances during the Cross-Validation Phase.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Validation set: This data set is used to compare the performances of the prediction algorithms that were created based on the training set. We choose the algorithm that has the best performance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Test set: Now we have chosen our preferred prediction algorithm but we don’t know yet how it’s going to perform on completely unseen real-world data. So, we apply our chosen prediction algorithm on our test set in order to see how it’s going to perform so we can have an idea about our algorithm’s performance on unseen data.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;p&gt;-It’s very important to keep in mind that skipping the test phase is not recommended, because the algorithm that performed well during the validation phase doesn’t really mean that it’s truly the best one, because the algorithms are compared based on the cross-validation set and its quirks and noises.&lt;/p&gt;

&lt;p&gt;-During the Test Phase, the purpose is to see how our final model is going to deal in the wild, so in case its performance is very poor we should repeat the whole process starting from the Training Phase.&lt;/p&gt;
</content>

			
				<category term="Machine Learning" />
			
			

			<published>2017-12-14T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/biavar/</id>
			<title>What is Bias-Variance Tradeoff</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/biavar/" rel="alternate" type="text/html" title="What is Bias-Variance Tradeoff" />
			<updated>2017-12-13T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/biavar/">&lt;p&gt;In short,&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;bias is how far away a model’s predictions are from true values,&lt;/li&gt;
  &lt;li&gt;variance is how scattered these predictions are among model iterations.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;row&quot;&gt;
    &lt;div class=&quot;medium-12 columns t30&quot;&gt;
    &lt;img src=&quot;/GreenData/images/bias-and-variance.jpg&quot; alt=&quot;&quot; /&gt;
    &lt;/div&gt;&lt;!-- /.medium-8.columns --&gt;

&lt;/div&gt;
&lt;!-- /.row --&gt;

&lt;blockquote&gt;

  &lt;p&gt;Error due to Bias: The error due to bias is taken as the difference between the expected (or average) prediction of our model and the correct value which we are trying to predict. Of course you only have one model so talking about expected or average prediction values might seem a little strange. However, imagine you could repeat the whole model building process more than once: each time you gather new data and run a new analysis creating a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. Bias measures how far off in general these models’ predictions are from the correct value.
&lt;cite&gt;Scott Fortmann-Roe&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;

  &lt;p&gt;Error due to Variance: The error due to variance is taken as the variability of a model prediction for a given data point. Again, imagine you can repeat the entire model building process multiple times. The variance is how much the predictions for a given point vary between different realizations of the model.
&lt;cite&gt;Scott Fortmann-Roe&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</content>

			
				<category term="Machine Learning" />
			
			

			<published>2017-12-13T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine-learning/generativeDiscriminative/</id>
			<title>Generative vs. Discriminatives</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine-learning/generativeDiscriminative/" rel="alternate" type="text/html" title="Generative vs. Discriminatives" />
			<updated>2017-12-12T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine-learning/generativeDiscriminative/">&lt;p&gt;The fundamental distinction between discriminative models and generative models is:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Discriminative models learn the (hard or soft) boundary between classes&lt;/li&gt;
  &lt;li&gt;Generative models model the distribution of individual classes&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;discriminative&quot;&gt;Discriminative&lt;/h4&gt;

&lt;p&gt;SVMs and decision trees are discriminative because they learn explicit boundaries between classes.&lt;/p&gt;

&lt;p&gt;SVM is a maximal margin classifier, meaning that it learns a decision boundary that maximizes the distance between samples of the two classes, given a kernel. The distance between a sample and the learned decision boundary can be used to make the SVM a “soft” classifier.&lt;/p&gt;

&lt;p&gt;Decision trees learn the decision boundary by recursively partitioning the space in a manner that maximizes the information gain (or another criterion).&lt;/p&gt;

&lt;h4 id=&quot;generative&quot;&gt;Generative&lt;/h4&gt;

&lt;p&gt;Generative models are typically specified as probabilistic graphical models. They offer rich representations of the independence relations in the dataset.&lt;/p&gt;

&lt;p&gt;Discriminative models do not offer such clear representations of relations between features and classes in the dataset. Instead of using resources to fully model each class, they focus on richly modeling the boundary between classes. Given the same amount of capacity (say, bits in a computer program executing the model), a discriminative model thus may yield more complex representations of this boundary than a generative model.&lt;/p&gt;

&lt;p&gt;When you are dealing with non-stationary distributions where the online test data may be generated by different underlying distributions than the training data, it is typically more straightforward to detect distribution changes and update a generative model accordingly than do this for a decision boundary in an SVM, especially if the online updates need to be unsupervised.&lt;/p&gt;

&lt;p&gt;To sum up:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Discriminative models&lt;/th&gt;
      &lt;th&gt;Generative models&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;learn the (hard or soft) boundary between classes&lt;/td&gt;
      &lt;td&gt;model the distribution of individual classes&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;generally do not function for outlier detection&lt;/td&gt;
      &lt;td&gt;generally function for outlier detection&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;do not offer clear representations of relations between features and classes in the dataset&lt;/td&gt;
      &lt;td&gt;offer rich representations of the independence relations in the dataset&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
</content>

			
				<category term="Machine-Learning" />
			
			
				<category term="ensemble" />
			

			<published>2017-12-12T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/topCoursera/</id>
			<title>Top 3 online courses</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/topCoursera/" rel="alternate" type="text/html" title="Top 3 online courses" />
			<updated>2017-12-11T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary></summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/topCoursera/">&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/specializations/probabilistic-graphical-models&quot;&gt;Probabilistic Graphical Models &lt;/a&gt;&lt;/p&gt;

&lt;p&gt;by Stanford&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/specializations/aml&quot;&gt;Advanced Machine Learning&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.coursera.org/specializations/deep-learning&quot;&gt;Deep Learning&lt;/a&gt;&lt;/p&gt;
</content>

			
				<category term="Machine Learning" />
			
			

			<published>2017-12-11T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/mlRecap/</id>
			<title>Top 4 families of machine learning algorithms</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/mlRecap/" rel="alternate" type="text/html" title="Top 4 families of machine learning algorithms" />
			<updated>2017-12-10T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary>Linear Model, Tree-based methods, k-Nearest Neighbors and Neural Nets</summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/mlRecap/">&lt;h2 id=&quot;no-free-lunch-theorem&quot;&gt;No Free Lunch Theorem&lt;/h2&gt;

&lt;p&gt;Basically, No Free Lunch Theorem states that there is no methods which outperform all others on all tasks.&lt;/p&gt;

&lt;p&gt;The reason behind is that every method relies on certain assumptions about data or task. If these assumptions fail, it will perform poorly.&lt;/p&gt;

&lt;p&gt;For us, this means that we cannot every competition with just a single algorithm. So it is important for us to have a clear mind map of various models based off different assumptions.&lt;/p&gt;

&lt;p&gt;Then let’s start getting familiar with the four popular families of machine learning algorithms.&lt;/p&gt;

&lt;h2 id=&quot;linear-model&quot;&gt;Linear Model&lt;/h2&gt;

&lt;p&gt;Linear models try to separate objects with a plane which divides space into two parts.&lt;/p&gt;

&lt;p&gt;With 2 sets of points, it is quite intuitive to separate them using a line. This approach can be generalized for a higher dimensional space. This is the main idea behind linear models.&lt;/p&gt;

&lt;p&gt;Logistic regression or SVM are all linear models withdifferent loss functions.&lt;/p&gt;

&lt;h3 id=&quot;linear-models-are-especially-good-for-sparse-high-dimensional-data&quot;&gt;Linear models are especially good for sparse high dimensional data.&lt;/h3&gt;

&lt;h2 id=&quot;tree-based-methods&quot;&gt;Tree-Based Methods&lt;/h2&gt;

&lt;p&gt;In general, tree-based models are very powerful and can be a good default method for tabular data.&lt;/p&gt;

&lt;p&gt;Intuitively, a single decision tree can be considered as dividing space into boxes and approximating data with a constant inside of these boxes. It uses divide-and-conquer approach to recur sub-split spaces into sub-spaces.&lt;/p&gt;

&lt;p&gt;The way of true axis splits and corresponding constants produces several approaches for building decision trees. Moreover, such trees can be combined together in a lot of ways. All this leads to a wide variety of tree-based algorithms, most famous of them being random forest and Gradient Boosted Decision Trees.&lt;/p&gt;

&lt;p&gt;Scikit-Learn contains quite good implementation of random forest. For gradient boost decision trees, you may find XGBoost and LightGBM with higher speed and accuracy.&lt;/p&gt;

&lt;h2 id=&quot;k-nearest-neighbors-k-nn&quot;&gt;k-Nearest Neighbors (k-NN)&lt;/h2&gt;

&lt;p&gt;The intuition behind k-NN is very simple. Closer objects will likely to have same labels.&lt;/p&gt;

&lt;h2 id=&quot;neural-networks&quot;&gt;Neural Networks&lt;/h2&gt;

&lt;p&gt;Neural Nets is a special class of machine learning models, which deserve a separate topic.&lt;/p&gt;
</content>

			
				<category term="Machine Learning" />
			
			
				<category term="overview" />
			

			<published>2017-12-10T00:00:00+08:00</published>
		</entry>
	
		<entry>
			<id>https://howtoanalyse.github.io/GreenData/machine%20learning/nn/</id>
			<title>Top 4 families of machine learning algorithms</title>
			<link href="https://howtoanalyse.github.io/GreenData/machine%20learning/nn/" rel="alternate" type="text/html" title="Top 4 families of machine learning algorithms" />
			<updated>2017-12-09T00:00:00+08:00</updated>

			
				
				<author>
					
						<name>Sara</name>
					
					
					
				</author>
			
			<summary>Linear Model, Tree-based methods, k-Nearest Neighbors and Neural Nets</summary>
			<content type="html" xml:base="https://howtoanalyse.github.io/GreenData/machine%20learning/nn/">&lt;p&gt;The common back propagation used in neural networks is to calculate gradient of the loss function with respect to the parameters of the network&lt;/p&gt;
</content>

			
				<category term="Machine Learning" />
			
			
				<category term="overview" />
			

			<published>2017-12-09T00:00:00+08:00</published>
		</entry>
	
</feed>